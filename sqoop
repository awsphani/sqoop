https://www.youtube.com/watch?v=icIj5v0BhHc&list=PLf0swTFhTI8rDQXfH8afWtGgpOTnhebDx
https://www.youtube.com/watch?v=olVVbKsbOzU&list=PLf0swTFhTI8qaFYQyR99wchg1Kx_BFXSg
https://www.youtube.com/watch?v=7IrXZZDSvBc&list=PLf0swTFhTI8qaFYQyR99wchg1Kx_BFXSg&index=4

sqoop user guide 1.4.6

$ mysql -u retail_user -h ms.itversity.com -p
password:  itversity

$sqoop version
$sqoop help list-databases;
$sqoop list-databases \
--connect jdbc:mysql://ms.itversity.com:3306 \
--username retail_user \
--password itversity or --password-file /user/pkum60/data/pwd.txt

$sqoop list-tables \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity 

$sqoop eval \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--query "select * from orders limit 10"

to check permission if we can insert using eval
$sqoop eval \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--query "insert into orders values(100000,\"2017-10-31 00:00:00:0\",\"DUMMY\" )"

hadoop fs -rm -R /user/awsphani/sqoop_import/retail_db/order_items;


$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--targer-dir /user/awsphani/sqoop_import/retail_db/order_items or --warehouse-dir /user/awsphani/sqoop_import/retail_db

--targer-dir =>dir should notexists, --warehouse-dir => will create subdir with tablename in the path provided 

validate:

hadoop fs -ls /user/awsphani/sqoop_import/retail_db/order_items;

hadoop fs -tail /user/awsphani/sqoop_import/retail_db/order_items/part-m-00001;

default delimiter is comma ,default --num-mappers is 4

hadoop fs -rm -R /user/awsphani/sqoop_import/retail_db/order_items; or use --delete-target-dir


$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--num-mappers 1
--delete-target-dir


exec life cycle,

sqoop cmd will be converted to mapreduce and executes,
1.will do select with limit 1 to know the columns and data types and creates .java file with that info
then it will count no of recs, checks min and max bounds using pks, the divide recs using no mappers 4 default,

--append will append the new file 


$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--num-mappers 1
--append


--append and --delete-target-dir are mutually exclusive;


if we need to import all tables from db ,use --warehouse-dir

$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--num-mappers 1 \
--delete-target-dir

create table order_items_nopk as select * from order_items;


$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items_nopk \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--delete-target-dir


//if no pk, sqoop will not know how to split the the data for the 4 threads and complain so we have have to use --split-by to define the col
//spliy-by column should be indexed,else it will do table full scan, as col is indexed it will scan only boundarys as per thread
//values in the field should be sparse(shud have as many recs as possible) and often seq generated or evenly incremented
//should not have null values


$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items_nopk \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--delete-target-dir
--split-by order_item_order_id

below will fail as spliy-by non-numeric fld, so have to use -Dord.apache.sqoop.splitter.allow_text_splitter=true 

$sqoop import \
-Dord.apache.sqoop.splitter.allow_text_splitter=true 
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table orders \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--delete-target-dir
--split-by order_status

auto-reset num-mappers to 1 if there are no Pk on the table if we dont use split-by column


$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items_nopk \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--delete-target-dir
--autoreset-to-one-mapper

file formats --as-textfile ,--as-avrodatafile (json binary file), --as-parquetfile (coumunar),--as-sequencefile ( binary file)

$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--num-mappers 2 \
--delete-target-dir \
--as-parquetfile

compressing the data-Gzip,deflate,snappy... --compress default for Gzip

$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--num-mappers 2 \
--delete-target-dir \
--as-textfile \
--compress

to use diff compresion
cd etc/hadoop/conf ->core-site.xml->search for avialble codecs, those jar files needed for cdecs should be present in right paths


$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--num-mappers 2 \
--delete-target-dir \
--as-textfile \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec


Boundary query: by default sqoop will get min and max values of the key to get boundaries to split the data,
we can specify the boundary to avoid the sqoop to compute it, to make faster,
below i want to consider order_item_id only from 100000

$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--boundary-query 'select min(order_item_id),max(order_item_id) from order_items where order_item_id >99999'

we can give min, max vals directly to reduce min max calculations.

$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--boundary-query 'select 100000, 17219'

Selecting specific columns from table, query; table and/or columns is  mutually exclusive with columns


$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--columns order_item_order_id,order_item_id,order_item_subtotal \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--num-mappers 2 \
--delete-target-dir \

'select o.*,sum(oi.order_item_subtotal) order_revenue
from orders o join order_items oi
on o.order_id=oi.order_item_order_id
group by o.order_id,o.order_date,o.order_customer_id,o.order_status'

--query => table or columns cannot be used, as below num-mappers is more than 1 we have specify split-by
and we have to specify target-dir with full path as table name cannot be used with query
our query must include the token $CONDITIONS which each Sqoop process will replace with a unique condition expression. 
You must also select a splitting column with --split-by.

$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--query 'select o.*,sum(oi.order_item_subtotal) order_revenue \
from orders o join order_items oi \
on o.order_id=oi.order_item_order_id \
group by o.order_id,o.order_date,o.order_customer_id,o.order_status' WHERE $CONDITIONS' \
--split-by order_id
--target-dir /user/awsphani/sqoop_import/retail_db/orders_revenue \
--num-mappers 2 \
--delete-target-dir \

delimiters and handling nulls:

mysql -u hr_user -h ms.itversity.com -p

$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \hr_db \
--username hr_user \
--password itversity \
--table employees \
--warehouse-dir /user/awsphani/sqoop_import/hr_db \

hadoop fs -get /user/awsphani/sqoop_import/hr_db/employees .

employees has col commision_pct which can be null, so we have to set to -1 if commision is null
null-string and null-non-string can be used to replace the null values in the record
columns/flds are delimted by , by default; to change use --fields-terminated-by
--enclosed-by to enclose columns/flds
--optionally-enclosed-by , if default delimitter is used and address col has , inside data 
we use optionally-enclosed-by for the address field

****below very important 

$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \hr_db \
--username hr_user \
--password itversity \
--table employees \
--warehouse-dir /user/awsphani/sqoop_import/hr_db \
--null-non-string -1 
--fields-terminated-by "\t" \              => for ascii null values,  "\000"
--lines-terminated-by ":"


incremental loads using query, where, alternative


$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--query "select * from orders where \$CONDITIONS and order_date like '2014-01%'" \
--split-by order_id \
--target-dir /user/awsphani/sqoop_import/retail_db/orders \
--num-mappers 2 \
--append

$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table orders \ 
--where "order_date like '2014-01%'" \
--split-by order_id \
--target-dir /user/awsphani/sqoop_import/retail_db/orders/  \
,--num-mappers 2 \
--append


Argument	Description
--check-column (col)	Specifies the column to be examined when determining which rows to import. (the column should not be of type CHAR/NCHAR/VARCHAR/VARNCHAR/ LONGVARCHAR/LONGNVARCHAR)
--incremental (mode)	Specifies how Sqoop determines which rows are new. Legal values for mode include append and lastmodified.
--last-value (value)	Specifies the maximum value of the check column from the previous import.

Sqoop supports two types of incremental imports: append and lastmodified. 
You can use the --incremental argument to specify the type of incremental import to perform.

You should specify append mode when importing a table where new rows are continually being added with increasing row id values. You 
specify the column containing the row’s id with --check-column. 
Sqoop imports rows where the check column has a value greater than the one specified with --last-value.



$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--target-dir /user/awsphani/sqoop_import/retail_db/orders  \
--num-mappers 2 \
--split-by order_id \
--table orders \
--check-column order_date \    => should not be char,varchar,string feilds, normally should be numeric fld   
--incremental-append
--last-value '2014-01-30'


hive-import will create table if not exists, default delimiter is ^A , for sqoop ,



$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--hive-import \
--hive-database awsphani_sqoop_import \
--hive-table order_items \
--num-mappers 2 \
--delete-target-dir \

if we run again above cmd, it will append the data to exisiting table

--hive-overwrite will drop the existing table and recreate it

$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--hive-import \
--hive-database awsphani_sqoop_import \
--hive-table order_items \
--hive-overwrite \
--num-mappers 2 \
--delete-target-dir \


if table hive exists, below will fail ; --create-hive-table => default is false, so will fail if hive table exists

--create-hive-table and --hive-overwrite are mutually exclusive

$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table order_items \
--hive-import \
--hive-database awsphani_sqoop_import \
--hive-table order_items \
--create-hive-table \
--num-mappers 2 \


$sqoop import \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--table orders \
--hive-import \
--hive-database awsphani_sqoop_import \
--hive-table orders \
--create-hive-table \
--num-mappers 2 \


hive import will import to staging area (/user/awsphani/order_items)before writing to final loc, if above fails because bcoz of table exixts,
need to clean the staging , hadoop fs -rm /user/awsphani/order_items and run the hive import with overwrite option


import ALL TABLES;

limitations:
--warehouse-dir is mandatory
better to use autoreset-to-one-mapper
cannot specify many args like --query, --cols, --where which does filtering and transformations on the data
incremental import not possible


$sqoop import-all-tables \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_db \
--username retail_user \
--password itversity \
--warehouse-dir /user/awsphani/sqoop_import/retail_db \
--autoreset-to-one-mapper

in hive:
create table daily_revenue as
select order_date,sum(order_item_subtotal) daily_revenue
from orders join order_items on
order_id = order_items_order_id
where order_date like '2013-07%'
group by order_date;

get location: show formatted table daily_revenue => /apps/hive/warehouse/awsphani_sqoop_import.db/daily_revenue
 in hive default delimiter is ^A => "\001"

sqoop export

in mysql:  create table daily_revenue( order_date varchar(30),revenue float);

sqoop export \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_export \
--username retail_user \
--password itversity \
--export-dir /apps/hive/warehouse/awsphani_sqoop_import.db/daily_revenue \
--table daily_revenue \
--input-fields-terminated-by "\001"

column mapping


sqoop export \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_export \
--username retail_user \
--export-dir /apps/hive/warehouse/awsphani_sqoop_import.db/daily_revenue \
--table daily_revenue \
--columns order_date,revenue    =>mysql target flds
--input-fields-terminated-by "\001"

--call <SP>, --table <tablename> are mutually exclusive

updating the data

if we have PK on order_date in target mysql, if we run above export 2 times, 2nd time it will fail bcoz of PK

to update the exisiting rec --update-key -> will update only records mathing the PK and ignore others


sqoop export \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_export \
--username retail_user \
--password itversity \
--export-dir /apps/hive/warehouse/awsphani_sqoop_import.db/daily_revenue \
--table daily_revenue \
--update-key  order_date \
--input-fields-terminated-by "\001"
--num-mappers 1

upsert --update-mode allowinsert =>update matching pks else insert

sqoop export \
--connect jdbc:mysql://ms.itversity.com:3306 \retail_export \
--username retail_user \
--password itversity \
--export-dir /apps/hive/warehouse/awsphani_sqoop_import.db/daily_revenue \
--table daily_revenue \
--update-key  order_date \
--update-mode allowinsert
--input-fields-terminated-by "\001"
--num-mappers 1



















